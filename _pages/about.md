---
permalink: /
title: "Edward Fish, PhD"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

üëã

I'm Ed, a Senior Research Fellow at the Centre for Vision, Speech and Signal Processing (CVSSP) at the University of Surrey, where I work on computer vision for accessibility with [Professor Richard Bowden](https://www.surrey.ac.uk/people/richard-bowden) in the Cognitive Vision Group.

I recently completed my PhD in Efficient Multi-Modal Video Understanding, supervised by [Dr. Andrew Gilbert](https://www.surrey.ac.uk/people/andrew-gilbert). Currently I am focussed on research in Automated Sign Language Translation as part of the EPSRC project [**Sign GPT**](https://www.bbc.co.uk/news/articles/c4g9rd4g8w2o) alongside work on AI for efficient Sign Language Annotation funded by Google.org.

Prior to my PhD I worked for a number of social enterprises focussed on improving access to careers in computing and the creative industries. I'm always happy to help review CV's, university/college applications, and provide advice where I can. Simply drop me an email or a message on LinkedIn. 

You can find my [publications](/publications/) and [CV](/cv/) here.

---

## üì¢ News

* **August 2025:** I obtained my BSL 101-103 certification. Now studying towards level 2.
* **August 2025:** I'm chairing the BMVA one day symposium on AI for Sign Language Translation, Production, and Linguistics on December 10th. Register to present or attend [here](https://www.bmva.org/meetings/25-12-10-Sign.html) We will announce keynotes soon. 
* **July 2025:** Our paper, "VALLR: Visual ASR Language Model for Lip Reading", is accepted to **ICCV 2025**!
* **July 2025:** Our paper, "Prompt Learning with Optimal Transport for Few-Shot Temporal Action Localization", is accepted to **ICCV workshop - CLVL 2025** .
* **June 2025:** I'm at **CVPR 2025** co-chairing the Sign Language Recognition, Recognition, Translation, and Production (SLRTP) workshop. You can read our paper on the competition we ran [here](https://arxiv.org/abs/2508.06951)
* **May 2025:** Code and paper for "Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware Sign Language Translation" is available online [here](https://github.com/ed-fish/geo-sign) (Under review ü§û)

---

## Current PhD Students

**Marshall Thomas**: "Integrating Non-Manual Features for Robust Sign Language Translation" (Co-Supervisor with Prof. Richard Bowden)

**[Karahan ≈ûahin](https://github.com/karahan-sahin)**: "Unified representations for Sign Language Translation and Production" (Co-Supervisor with Prof. Richard Bowden)

---


## üìù 2025 Publications

This is a selection of my recent papers from this year. For a complete list, please see my [publications page](/publications/).

<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    <div class="archive__item-thumb">
      <a href="/publications/2025-geo-sign"><img src="/images/geosign.png" alt="Thumbnail for Geo-Sign paper"></a>
    </div>
    <div class="archive__item-content">
      <h3 class="archive__item-title" itemprop="headline">
        <a href="/publications/2025-geo-sign">Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically-Aware Sign-Language Translation</a>
      </h3>
      <p class="archive__item-meta">Authors: <strong>E. Fish</strong>, R. Bowden</p>
      <p class="archive__item-excerpt" itemprop="description">Our pose-only method beats state-of-the-art pixel approaches by injecting hierarchical structure into a language model using hyperbolic geometry via a novel regulariser. </p>
    </div>
  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    <div class="archive__item-thumb">
      <a href="/publications/2025-vallr"><img src="/images/vallr.png" alt="Thumbnail for VALLR paper on lip reading"></a>
    </div>
    <div class="archive__item-content">
      <h3 class="archive__item-title" itemprop="headline">
        <a href="/publications/2025-vallr">VALLR: Visual ASR Language Model for Lip Reading</a>
      </h3>
      <p class="archive__item-meta"><strong>ICCV 2025</strong></p>
      <p class="archive__item-meta">Authors: M. Thomas, <strong>E. Fish</strong>, R. Bowden.</p>
      <p class="archive__item-excerpt" itemprop="description">Achieves state-of-the-art results in lip reading with 99% less training data by deconstructing the problem into phoneme recognition and sentence reconstruction.</p>
    </div>
  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    <div class="archive__item-thumb">
      <a href="/publications/2025-plot-tal"><img src="/images/plot-tal.png" alt="Thumbnail for PLOT TAL paper"></a>
    </div>
    <div class="archive__item-content">
      <h3 class="archive__item-title" itemprop="headline">
        <a href="/publications/2025-plot-tal">PLOT TAL: Prompt Learning with Optimal Transport for Few Shot Temporal Action Localization</a>
      </h3>
      <p class="archive__item-meta">Authors: <strong>E. Fish</strong>, A. Gilbert.</p>
        <p class="archive__item-meta"><strong>ICCV 2025</strong> (CLVL Workshop) </p>
        <p class="archive__item-excerpt" itemprop="description">A novel approach for few-shot temporal action localization, leveraging prompt learning and optimal transport.</p>
    </div>
  </article>
</div>
