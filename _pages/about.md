---
permalink: /
title: "Edward Fish, PhD"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hello there! üëã

I'm Ed Fish, a Senior Research Fellow at the Centre for Vision, Speech and Signal Processing (CVSSP) at the University of Surrey, where I work on computer vision for accessibility with [Professor Richard Bowden](https://www.surrey.ac.uk/people/richard-bowden) in the Cognitive Vision Group.

I recently completed my PhD in Efficient Multi-Modal Video Understanding, supervised by Dr. Andrew Gilbert. Currently I am focussed on applied research in Automated Sign Language Translation as part of the EPSRC project "Sign GPT" alongside work on AI for efficient Sign Language Annotation funded by Google.org.

If you are interested in a PhD in our lab, and are looking for a supervisor  I would be happy to chat with you about opportunities.

You can find my [publications](/publications/) and [CV](/cv/) here.

---

## üì¢ News
* **August 2025:** I obtained my BSL Level 101-103 certificate. Now studying towards level 2.
* **August 2025:** I'm chairing the BMVA one day symposium on AI for Sign Language Translation, Production, and Linguistics on December 10th. Register to present [here](https://www.bmva.org/meetings/25-12-10-Sign.html) 
* **July 2025:** Our paper, "VALLR: Visual ASR Language Model for Lip Reading", was accepted to **ICCV 2025**!
* **July 2025:** Our paper, "Prompt Learning with Optimal Transport for Few-Shot Temporal Action Localization", was accepted to **ICCV workshop - CLVL 2025** .
* **June 2025:** At **CVPR 2025** chairing the Sign Language Recognition, Recognition, Translation, and Production (SLRTP) workshop.
* **May 2025:** Code and paper for "Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware Sign Language Translation" is available online [here](https://github.com/ed-fish/geo-sign)

---

## Current PhD Students

**Marshall Thomas**: "Integrating Non-Manual Features for Robust Sign Language Translation" (Co-Supervisor with Prof. Richard Bowden)

**[Karahan ≈ûahin](https://github.com/karahan-sahin)**: "Unified representations for Sign Language Translation and Production" (Co-Supervisor with Prof. Richard Bowden)


## üìù Selected Publications

This is a selection of my recent work. For a complete list, please see my [publications page](/publications/).

<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    <div class="archive__item-thumb">
      <a href="/publications/2025-geo-sign"><img src="/images/geo-sign-thumb.jpg" alt="Thumbnail for Geo-Sign paper"></a>
    </div>
    <div class="archive__item-content">
      <h3 class="archive__item-title" itemprop="headline">
        <a href="/publications/2025-geo-sign">Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically-Aware Sign-Language Translation</a>
      </h3>
      <p class="archive__item-meta">Authors: <strong>E. Fish</strong>, [Author 2], [Author 3]</p>
      <p class="archive__item-excerpt" itemprop="description">Our pose-only method beats state-of-the-art pixel approaches by injecting hierarchical structure into a language model using hyperbolic geometry, setting new records on the CSL-Daily benchmark.</p>
    </div>
  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    <div class="archive__item-thumb">
      <a href="/publications/2025-vallr"><img src="/images/vallr-thumb.jpg" alt="Thumbnail for VALLR paper on lip reading"></a>
    </div>
    <div class="archive__item-content">
      <h3 class="archive__item-title" itemprop="headline">
        <a href="/publications/2025-vallr">VALLR: Visual ASR Language Model for Lip Reading</a>
      </h3>
      <p class="archive__item-meta">Authors: [Author 1], <strong>E. Fish</strong>, [Author 3]. **Accepted to ICCV 2025!**</p>
      <p class="archive__item-excerpt" itemprop="description">Achieves state-of-the-art results in lip reading with 99% less training data by deconstructing the problem into phoneme recognition and sentence reconstruction.</p>
    </div>
  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    <div class="archive__item-thumb">
      <a href="/publications/2025-plot-tal"><img src="/images/plottal-thumb.jpg" alt="Thumbnail for PLOT TAL paper"></a>
    </div>
    <div class="archive__item-content">
      <h3 class="archive__item-title" itemprop="headline">
        <a href="/publications/2025-plot-tal">PLOT TAL: Prompt Learning with Optimal Transport for Few Shot Temporal Action Localization</a>
      </h3>
      <p class="archive__item-meta">Authors: [Author 1], [Author 2], <strong>E. Fish</strong>. **Accepted to ICCV (CLVL Workshop) 2025!**</p>
      <p class="archive__item-excerpt" itemprop="description">A novel approach for few-shot temporal action localization, leveraging prompt learning and optimal transport.</p>
    </div>
  </article>
</div>

---


---

<p style="font-size: 0.8em; color: #666;">
This website is built with the <a href="https://github.com/academicpages/academicpages.github.io" target="_blank" rel="noopener">Academic Pages</a> template and hosted on GitHub Pages.
</p>
