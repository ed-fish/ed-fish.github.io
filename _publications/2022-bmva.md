---
title: "Two-Stream Transformer Architecture for Long Video Understanding"
collection: publications
category: conference
permalink: /publication/2022-stan
excerpt: 'This paper introduces STAN, an efficient two-stream Spatio-Temporal Attention Network that effectively models long videos for classification tasks on a single GPU.'
date: 2022-08-02
venue: 'British Machine Vision Conference (BMVC) 2022'
paperurl: 'https://arxiv.org/abs/2208.01753'
citation: 'Fish, E., Weinbren, J., & Gilbert, A. (2022). &quot;Two-Stream Transformer Architecture for Long Video Understanding.&quot; <i>Proceedings of the British Machine Vision Conference (BMVC)</i>.'
---

## Abstract

Pure vision transformer architectures are highly effective for short video classification and action recognition tasks. However, due to the quadratic complexity of self attention and lack of inductive bias, transformers are resource intensive and suffer from data inefficiencies. Long form video understanding tasks amplify data and memory efficiency problems in transformers making current approaches unfeasible to implement on data or memory restricted domains. This paper introduces an efficient Spatio-Temporal Attention Network (STAN) which uses a two-stream transformer architecture to model dependencies between static image features and temporal contextual features. Our proposed approach can classify videos up to two minutes in length on a single GPU, is data efficient, and achieves SOTA performance on several long video understanding tasks.

---

<details>
<summary>BibTeX</summary>
<pre>
@inproceedings{fish2022stan,
  title={Two-Stream Transformer Architecture for Long Video Understanding},
  author={Fish, Edward and Weinbren, Jon and Gilbert, Andrew},
  booktitle={Proceedings of the British Machine Vision Conference},
  year={2022}
}
</pre>
</details>
